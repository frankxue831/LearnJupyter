{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc121e30a2defb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ad946",
   "metadata": {},
   "source": [
    "extrct data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2174a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Setup logging with a detailed format\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ABSDataProcessor:\n",
    "    \"\"\"Process ABS (Asset-Backed Securities) data from Excel files.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dir: str = \"data/total_data_demo1\",\n",
    "        output_dir: str = \"processed_data\",\n",
    "        structured_dir: str = \"structured_data\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the data processor.\n",
    "\n",
    "        Args:\n",
    "            input_dir: Directory containing input Excel files\n",
    "            output_dir: Directory for processed individual files\n",
    "            structured_dir: Directory for combined/structured output\n",
    "        \"\"\"\n",
    "        self.input_dir = Path(input_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.structured_dir = Path(structured_dir)\n",
    "        self.raw_data: Dict[str, pd.DataFrame] = {}\n",
    "        self.processed_data: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "        # Create necessary directories\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.structured_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def normalize_columns(self, columns: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Normalize column names for consistent processing.\n",
    "\n",
    "        Args:\n",
    "            columns: List of column names to normalize\n",
    "\n",
    "        Returns:\n",
    "            List of normalized column names\n",
    "        \"\"\"\n",
    "        normalized = []\n",
    "        for c in columns:\n",
    "            if not isinstance(c, str):\n",
    "                c = str(c)\n",
    "            c_norm = c.strip()\n",
    "            c_norm = re.sub(r'\\s+', '_', c_norm)  # Replace spaces with underscores\n",
    "            c_norm = re.sub(r'[^\\w\\s-]', '', c_norm)  # Remove special characters\n",
    "            normalized.append(c_norm)\n",
    "        return normalized\n",
    "\n",
    "    def process_excel(self, file_path: str) -> Optional[Dict[str, pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Process Excel file sheets and clean data.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to the Excel file\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping sheet names to processed DataFrames, or None if processing fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            sheets = pd.read_excel(file_path, sheet_name=None, engine='openpyxl')\n",
    "            processed_data = {}\n",
    "\n",
    "            for sheet_name, df in sheets.items():\n",
    "                # Normalize column names\n",
    "                df.columns = self.normalize_columns(df.columns)\n",
    "\n",
    "                # Drop fully empty rows and columns\n",
    "                df = df.dropna(how='all')\n",
    "                df = df.dropna(axis=1, how='all')\n",
    "\n",
    "                # Handle Unnamed columns if present\n",
    "                if 'Unnamed_0' in df.columns or any(col.startswith('Unnamed') for col in df.columns):\n",
    "                    unnamed_cols = [col for col in df.columns if col.startswith('Unnamed')]\n",
    "                    for unnamed_col in unnamed_cols:\n",
    "                        parts = sheet_name.split('_', 1)\n",
    "                        new_col_name = parts[1] if len(parts) > 1 else f\"value_{unnamed_col.lower()}\"\n",
    "                        df.rename(columns={unnamed_col: new_col_name}, inplace=True)\n",
    "\n",
    "                if df.empty:\n",
    "                    logger.info(f\"Sheet '{sheet_name}' is empty, skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Add metadata columns\n",
    "                df['source_file'] = Path(file_path).name\n",
    "                df['sheet_name'] = sheet_name\n",
    "                df['process_date'] = pd.Timestamp.now().strftime('%Y-%m-%d')\n",
    "\n",
    "                logger.info(f\"Processing sheet: {sheet_name}\")\n",
    "                processed_data[sheet_name] = df\n",
    "\n",
    "            return processed_data\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing file '{file_path}': {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def save_data(\n",
    "        self,\n",
    "        processed_data: Dict[str, pd.DataFrame],\n",
    "        output_dir: Path,\n",
    "        prefix: str\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Save processed data to CSV and JSON files.\n",
    "\n",
    "        Args:\n",
    "            processed_data: Dictionary of processed DataFrames\n",
    "            output_dir: Directory to save the files\n",
    "            prefix: Prefix for the output filenames\n",
    "        \"\"\"\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for sheet_name, df in processed_data.items():\n",
    "            try:\n",
    "                # Clean sheet name for filename\n",
    "                cleaned_sheet_name = re.sub(r'\\W+', '_', sheet_name)\n",
    "                base_name = f\"{prefix}_{cleaned_sheet_name}\"\n",
    "\n",
    "                # Save as CSV\n",
    "                csv_file = output_dir / f\"{base_name}.csv\"\n",
    "                df.to_csv(csv_file, index=False, encoding='utf-8-sig')\n",
    "                logger.info(f\"Saved CSV: {csv_file}\")\n",
    "\n",
    "                # Save as JSON\n",
    "                json_file = output_dir / f\"{base_name}.json\"\n",
    "                df.to_json(json_file, orient='records', force_ascii=False, indent=2)\n",
    "                logger.info(f\"Saved JSON: {json_file}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error saving sheet '{sheet_name}': {str(e)}\")\n",
    "\n",
    "    def process_directory(self, input_dir: Path, output_dir: Path) -> None:\n",
    "        \"\"\"\n",
    "        Process all Excel files in the directory structure.\n",
    "\n",
    "        Args:\n",
    "            input_dir: Root directory containing Excel files\n",
    "            output_dir: Directory to save processed files\n",
    "        \"\"\"\n",
    "        combined_data = {}\n",
    "\n",
    "        for root, _, files in os.walk(input_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.xlsx') and \"资产池特征分布\" in file:\n",
    "                    file_path = Path(root) / file\n",
    "                    relative_path = Path(root).relative_to(input_dir)\n",
    "                    output_subdir = output_dir / relative_path\n",
    "                    prefix = file_path.parent.name\n",
    "\n",
    "                    logger.info(f\"Processing file: {file_path}\")\n",
    "                    processed_data = self.process_excel(str(file_path))\n",
    "\n",
    "                    if processed_data:\n",
    "                        # Save individual files\n",
    "                        self.save_data(processed_data, output_subdir, prefix)\n",
    "\n",
    "                        # Accumulate data for combined files\n",
    "                        for sheet_name, df in processed_data.items():\n",
    "                            if sheet_name not in combined_data:\n",
    "                                combined_data[sheet_name] = []\n",
    "                            combined_data[sheet_name].append(df)\n",
    "\n",
    "        # Create combined files\n",
    "        self._save_combined_data(combined_data)\n",
    "\n",
    "    def _save_combined_data(self, combined_data: Dict[str, List[pd.DataFrame]]) -> None:\n",
    "        \"\"\"\n",
    "        Save combined data files.\n",
    "\n",
    "        Args:\n",
    "            combined_data: Dictionary mapping sheet names to lists of DataFrames\n",
    "        \"\"\"\n",
    "        combined_dir = self.structured_dir / \"combined\"\n",
    "        combined_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for sheet_name, dfs in combined_data.items():\n",
    "            try:\n",
    "                # Combine all DataFrames for this sheet type\n",
    "                combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "                # Clean sheet name for filename\n",
    "                cleaned_name = re.sub(r'\\W+', '_', sheet_name)\n",
    "                output_file = combined_dir / f\"combined_{cleaned_name}_distribution.csv\"\n",
    "\n",
    "                # Save combined file\n",
    "                combined_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "                logger.info(f\"Saved combined file: {output_file}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error creating combined file for '{sheet_name}': {str(e)}\")\n",
    "\n",
    "class DataOrganizer:\n",
    "    \"\"\"Organizes processed data into structured formats.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: str = \"structured_data/combined\"):\n",
    "        \"\"\"\n",
    "        Initialize the data organizer.\n",
    "\n",
    "        Args:\n",
    "            data_dir: Directory containing combined CSV files\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.raw_data: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "        # Ensure directory exists\n",
    "        self.data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def load_raw_data(self) -> None:\n",
    "        \"\"\"Load all CSV files from the data directory.\"\"\"\n",
    "        for file_path in self.data_dir.glob(\"combined_*.csv\"):\n",
    "            try:\n",
    "                # Extract distribution type from filename\n",
    "                dist_type = re.search(r'combined_(.+)_distribution\\.csv', file_path.name)\n",
    "                if dist_type:\n",
    "                    key = dist_type.group(1)\n",
    "\n",
    "                    # Read CSV and validate required columns\n",
    "                    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "                    if 'source_file' not in df.columns:\n",
    "                        logger.warning(f\"Missing 'source_file' column in {file_path}\")\n",
    "                        continue\n",
    "\n",
    "                    # Clean the data\n",
    "                    df = df.dropna(subset=['source_file'])\n",
    "\n",
    "                    self.raw_data[key] = df\n",
    "                    logger.info(f\"Loaded {key} distribution data\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading {file_path}: {str(e)}\")\n",
    "\n",
    "    def _extract_product_details(self, filename: str) -> dict:\n",
    "        \"\"\"\n",
    "        Extract product year and series from filename.\n",
    "\n",
    "        Args:\n",
    "            filename: Source filename to extract details from\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing year and series information\n",
    "        \"\"\"\n",
    "        if pd.isna(filename):\n",
    "            return {'year': None, 'series': None}\n",
    "\n",
    "        match = re.search(r'(\\d{2})(\\dC)', str(filename))\n",
    "        if match:\n",
    "            return {\n",
    "                'year': f\"20{match.group(1)}\",\n",
    "                'series': match.group(2)\n",
    "            }\n",
    "        return {'year': None, 'series': None}\n",
    "\n",
    "    def organize_by_product(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Organize data by product (year and series).\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing organized data structure\n",
    "        \"\"\"\n",
    "        product_data = {}\n",
    "\n",
    "        for dist_type, df in self.raw_data.items():\n",
    "            # Skip empty DataFrames\n",
    "            if df.empty:\n",
    "                continue\n",
    "\n",
    "            # Process each row\n",
    "            for _, row in df.iterrows():\n",
    "                try:\n",
    "                    product = self._extract_product_details(row['source_file'])\n",
    "                    year = product['year']\n",
    "                    series = product['series']\n",
    "\n",
    "                    if not year or not series:\n",
    "                        continue\n",
    "\n",
    "                    # Build nested dictionary structure\n",
    "                    product_data.setdefault(year, {})\n",
    "                    product_data[year].setdefault(series, {})\n",
    "                    product_data[year][series].setdefault(dist_type, [])\n",
    "\n",
    "                    # Add row data\n",
    "                    row_dict = row.to_dict()\n",
    "                    product_data[year][series][dist_type].append(row_dict)\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing row in {dist_type}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        return product_data\n",
    "\n",
    "    def save_structured_data(self, output_dir: str = \"organized_data\") -> None:\n",
    "        \"\"\"\n",
    "        Save structured data to JSON files.\n",
    "\n",
    "        Args:\n",
    "            output_dir: Directory to save organized data files\n",
    "        \"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            # Get organized data\n",
    "            product_data = self.organize_by_product()\n",
    "\n",
    "            # Save by year and series\n",
    "            for year in product_data:\n",
    "                year_dir = output_path / year\n",
    "                year_dir.mkdir(exist_ok=True)\n",
    "\n",
    "                for series in product_data[year]:\n",
    "                    filename = f\"{year}_{series}_data.json\"\n",
    "                    file_path = year_dir / filename\n",
    "\n",
    "                    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(\n",
    "                            product_data[year][series],\n",
    "                            f,\n",
    "                            ensure_ascii=False,\n",
    "                            indent=2\n",
    "                        )\n",
    "                    logger.info(f\"Saved structured data: {file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving structured data: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Execute the complete data processing pipeline.\"\"\"\n",
    "    try:\n",
    "        # Initialize processor\n",
    "        processor = ABSDataProcessor(\n",
    "            input_dir=\"data/total_data_demo1\",\n",
    "            output_dir=\"processed_data\",\n",
    "            structured_dir=\"structured_data\"\n",
    "        )\n",
    "\n",
    "        # Process all Excel files\n",
    "        processor.process_directory(\n",
    "            input_dir=Path(\"data/total_data_demo1\"),\n",
    "            output_dir=Path(\"processed_data\")\n",
    "        )\n",
    "\n",
    "        # Initialize organizer\n",
    "        organizer = DataOrganizer(data_dir=\"structured_data/combined\")\n",
    "\n",
    "        # Load and organize the data\n",
    "        organizer.load_raw_data()\n",
    "        organizer.save_structured_data(output_dir=\"organized_data\")\n",
    "\n",
    "        logger.info(\"Data processing pipeline completed successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline execution failed: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
